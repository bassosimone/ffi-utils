# Measurement Kit FFI API

Measurement Kit exposes a simple C like API suitable to be used via
[FFI](https://en.wikipedia.org/wiki/Foreign_function_interface).

As of MK v0.9.0, this API is still experimental. You should probably not
use this API directly, rather you should use the high level wrappers that
we have implemented for several languages. Specifically:

- for Android and Java you should use our [Java API](
  https://github.com/measurement-kit/android-libs);

- for Golang you should use our [Golang API](
  https://github.com/measurement-kit/go-measurement-kit);

- for C++11 and better, and Objective C, you should use our
  [C++11 API](../nettest/nettest.hpp).

## Introduction and synopsis

Measurement Kit is a network measurement engine. By default, as mentioned,
it exposes a FFI friendly C like API. To use this API, include
`<measurement_kit/ffi.h>`. See also the API documentation available
at [codedocs.xyz/measurement-kit/measurement-kit](
https://codedocs.xyz/measurement-kit/measurement-kit).

The FFI API allows you to run network tests (nettests). In the context of this
API, network tests are called _tasks_ (but the name is probably poised to
change before v1.0.0). Example tasks are [OONI's Web Connectivity](
https://github.com/ooni/spec/blob/master/test-specs/ts-017-web-connectivity.md
) or the [Network Diagnostic Test](
https://github.com/ooni/spec/blob/master/test-specs/ts-022-ndt.md).

To _start_ a task you call `mk_task_start` by passing it specific
_settings_ as a serialized JSON string.  All settings are optional,
except for the `name` of the task.  Ideally, you should have high
level code where the settings are a class that gets serialized to
a JSON string. This is, for example, what we do in the higher-level
APIs mentioned above.

Once started, a task will emit _events_. There are several kind of
events, the most common of which is `"log"` that identifies a log
line emitted by the task. Another event is `"status.update.performance"`,
which provides network performance information while a task for
measuring network performance is being run. See below for more
information on the specific events.

The task runs in a separate thread and posts events on a queue. You extract
events from such queue using `mk_task_wait_for_next_event`. This is a _blocking_
function that returns when a new event is posted into the queue. To
process an event, use `mk_event_serialize` to obtain its JSON serialization,
then parse the JSON into some high level data structure, and process it. See
the [C++14](../nettest/nettest.hpp) events processing code to have an idea of
how this could be implemented. (Or, if we have already written an API that
does that works for your use case, perhaps just use such API.)

You should loop processing events until `mk_task_is_done` returns nonzero. At
that point, the task is done, and attempting to extract further events from
the queue with `mk_task_wait_for_next_event` will immediately return the dummy
`status.terminated` event (equivalent to `EOF` for the task queue).

Since the FFI API is basically a C API, you need to manually manage memory
by freeing events and the task, once you are done with them. To this end, use
respectively, `mk_event_destroy` and `mk_task_destroy`.

## Examples

You can find working examples of usage of the FFI API inside the
[example/ffi](../../example/ffi) directory. Another usage example of
the FFI API is [nettest.hpp](../nettest/nettest.hpp), where we wrap the
FFI API into a more-user-friendly C++11 interface.

## Tasks

The following tasks are defined (case matters):
{% for nettest in nettests %}
- `"{{ nettest.key.to_pascal_case() }}"`: [{{ nettest.docs | string | wordwrap }}](
  {{ nettest.reference_url }})
{% endfor %}
By following the links provided above, you can read detailed documentation on
the purpose of each task. The documentation also describes the JSON schema used
by each task to represent its results. As mentioned below, tasks results are
emitted as a serialized JSON under the `"measurement"` event.

## Settings

The task settings is a JSON like:

```JSON
{
{% for setting in settings %}  "{{ setting.key }}": {{ setting.base_type.example(setting.key | string) }}{{ ",\n" if not loop.end }}{% endfor %}}
```

The only mandatory key is `name`, which identifies the task. The possible
task names have already been described above.

All the other keys are optional. We have omitted the context of the `options`
field, which is described in great detail below. The following keys are
available:
{% for setting in settings %}
- `"{{ setting.key }}"`: ({{ setting.base_type.decl("docs") }})
{{ setting.docs | string | wordwrap }}
{% endfor %}
In the following sections, we will describe in greater detail the available
log levels, options, and events.

## Log levels

The available log levels are:
{% for log_level in log_levels %}
- `"{{ log_level.key.to_snake_case_upper() }}"`: {{ log_level.docs }}.
{% endfor %}
When you specify a log level in the settings, only messages with a log level
equal or greater than the specified one are emitted. For example, if you
specify `"INFO"`, you will only see `"ERR"`, `"WARNING"`, and `"INFO"` logs.

## Options

Options are actually settings. They are distinct in this specification for
implementation reasons. However, higher level APIs SHOULD present options and
settings as a single class containing all the values that can be set.

Options can be `string`, `float`, or `int`. There is no boolean type: we use
`int`s with boolean semantics in some cases, with the usual convention that `0`
means false and non-`0` (typically `1`) means true.

These are the available options that affect all nettests:
{% for option in options %}
- `"{{ option.key }}"`: ({{ option.base_type.decl("docs") }})
{{ option.docs | string | wordwrap }}
{% endfor %}
The following is an example JSON where all the common options that affect
all nettests have been listed along with their default value:

```JSON
{
{% for option in options %}  "{{ option.key }}": {{ option.base_type.default_value("docs") }}{{ ",\n" if not loop.end }}{% endfor %}}
```

We will now proceed to describe the options specific to each test.
{% for nettest in nettests if nettest.options is not none %}
### {{ nettest.key.to_pascal_case() }} options
{% for option in nettest.options %}
- `"{{ option.key }}"`: ({{ option.base_type.decl("docs") }})
{{ option.docs | string | wordwrap }} Default value: {{ option.base_type.default_value("docs") }}.{% endfor %}{% endfor %}

## Events

An event is a JSON object like:

```JSON
{
  "key": "<key>",
  "value": {}
}
```

Where `"value"` is a JSON object with an event-specific structure, and `"key"`
is a string. Below we describe all the possible event keys, along with the
"value" JSON structure. Unless otherwise specified, an event key can be emitted
an arbitrary number of times during the lifecycle of a task. Unless otherwise
specified, all the keys introduced below where added in MK v0.9.0.

The following events are defined:
{%for event in events %}
- `"{{ event.key }}"`: (object) [documentation](#{{ event.key|replace(".", "") }})
{% endfor %}
Below we provide a detailed description of each event.
{%for event in events %}
### {{ event.key }}

{{ event.docs | string | wordwrap }}

{% if event.attributes|length > 0 %}This event includes the following attributes:
{% for attribute in event.attributes %}
1. `"{{ attribute.key }}"`: ({{ attribute.base_type.decl("docs") }})
{{ attribute.docs | string | wordwrap }}
{% endfor %}{% endif %}
The JSON returned by this event is like:

```JSON
{
  "key": "{{ event.key }}",
  "value": { {% for attribute in event.attributes %} {{ "\n   " if loop.first }} "{{ attribute.key }}": {{ attribute.base_type.default_value("docs") }}{{ ",\n  " if not loop.last }}{% endfor %}
  }
}
```
{% endfor %}

## Task pseudocode

The following illustrates in pseudocode the operations performed by a task
once you call `mk_task_start`. It not 100% accurate; in particular, we have
omitted the code that generates most log messages. This pseudocode is meant to
help you understand how Measurement Kit works internally, and specifically how all
the settings described above interact together when you specify them for
running Measurement Kit tasks. We are using pseudo JavaScript because that
is the easiest language to show manipulation of JSON objects such as the
`settings` object.

As mentioned, a task run in its own thread. It first validate settings, then
it opens the logfile (if needed), and finally it waits in queue until other
possibly running tasks terminate. The `finish` function will be called when the
task is done, and will emit all the events emitted at the end of a task.

```JavaScript
function taskThread(settings) {
  emitEvent("status.queued", {})
  semaphore.Acquire()                 // blocked until my turn

  let finish = function(error) {
    semaphore.Release()               // allow another test to run
    emitEvent("status.end", {
      downloaded_kb: countDownloadedKb(),
      uploaded_kb: countUploadedKb(),
      failure: (error) ? error.AsString() : null
    })
  }

  if (!settingsAreValid(settings)) {
    emitEvent("failure.startup", {
      failure: "value_error",
    })
    finish("value_error")
    return
  }

  if (settings.log_filepath != "") {
    // TODO(bassosimone): we should decide whether we want to deal with the
    // case where we cannot write into the log file. Currently we don't.
    openLogFile(settings.log_filepath)
  }

  let task = makeTask(settings.name)

  emitEvent("status.started", {})


```

After all this setup, a task contacts the OONI bouncer, lookups the IP address,
the country code, the autonomous system number, and the resolver lookup. All
these information end up in the JSON measurement. Also, all these operations can
be explicitly disabled by setting the appropriate settings.

```JavaScript
  let test_helpers = test.defaultTestHelpers()
  if (!settings.options.no_bouncer) {
    if (settings.options.bouncer_base_url == "") {
      settings.options.bouncer_base_url = defaultBouncerBaseURL()
    }
    let error
    [test_helpers, error] = queryOONIBouncer(settings)
    if (error) {
      emitWarning(settings, "cannot query OONI bouncer")
      if (!settings.options.ignore_bouncer_error) {
        finish(error)
        return
      }
    }
  }

  // TODO(bassosimone): we should make sure the progress described here
  // is consistent with the one emitted by the real code.
  emitProgress(0.1, "contacted bouncer")

  let probe_ip = "127.0.0.1"
  if (settings.options.probe_ip != "") {
    probe_ip = settings.options.probe_ip
  } else if (!settings.options.no_ip_lookup) {
    let error
    [probe_ip, error] = lookupIP(settings)
    if (error) {
      emitEvent("failure.ip_lookup", {
        failure: error.AsString()
      })
      emitWarning(settings, "cannot lookup probe IP")
    }
  }

  let probe_asn = "AS0",
      probe_network_name = "Unknown"
  if (settings.options.probe_asn != "") {
    probe_asn = settings.options.probe_asn
  } else if (!settings.options.no_asn_lookup &&
             settings.options.geoip_asn_path != "") {
    let error
    [probe_asn, probe_network_name, error] = lookupASN(settings)
    if (error) {
      emitEvent("failure.asn_lookup", {
        failure: error.AsString()
      })
      emitWarning(settings, "cannot lookup probe ASN")
    }
  }

  let probe_cc = "ZZ"
  if (settings.options.probe_cc != "") {
    probe_cc = settings.options.probe_cc
  } else if (!settings.options.no_cc_lookup &&
             settings.options.geoip_country_path != "") {
    let error
    [probe_cc, error] = lookupCC(settings)
    if (error) {
      emitEvent("failure.cc_lookup", {
        failure: error.AsString()
      })
      emitWarning(settings, "cannot lookup probe CC")
    }
  }

  emitEvent("status.geoip_lookup", {
    probe_ip: probe_ip,
    probe_asn: probe_asn,
    probe_network_name: probe_network_name,
    probe_cc: probe_cc
  })

  emitProgress(0.2, "geoip lookup")

  // TODO(bassosimone): take decision wrt null vs. ""
  let resolver_ip = null
  if (!settings.options.no_resolver_lookup) {
    let error
    [resolver_ip, error] = lookupResolver(settings)
    if (error) {
      emitEvent("failure.resolver_lookup", {
        failure: error.AsString()
      })
      emitWarning(settings, "cannot lookup resolver IP")
    }
  }

  emitEvent("status.resolver_lookup", {
    resolver_ip: resolver_ip
  })
  emitProgress(0.3, "resolver lookup")
```

Then, Measurement Kit opens the report file on disk, which will contain
the measurements, each serialized on a JSON on its own line. It will also
contact the OONI bouncer and obtain a report-ID for the report.

```JavaScript
  if (!settings.options.no_file_report) {
    if (settings.output_filepath == "") {
      settings.output_filepath = makeDefaultOutputFilepath(settings);
    }
    let error = openFileReport(settings.output_filepath)
    if (error) {
      emitWarning(settings, "cannot open file report")
      finish(error)
      return
    }
  }

  let report_id
  if (!settings.options.no_collector) {
    if (settings.options.collector_base_url == "") {
      settings.options.collector_base_url = defaultCollectorBaseURL();
    }
    let error
    [report_id, error] = collectorOpenReport(settings)
    if (error) {
      emitWarning("cannot open report with OONI collector")
      emitEvent("failure.report_create", {
        failure: error.AsString()
      })
      if (!settings.options.ignore_open_report_error) {
        finish(error)
        return
      }
    } else {
      emitEvent("status.report_create", {
        report_id: report_id
      })
    }
  }

  emitProgress(0.4, "open report")
```

Then comes input processing. Measurement Kit assembles a list of inputs to
process. If the test do not take any input, we fake a single input entry
consisting of the empty string, implying that this test needs to perform just
a single iteration. (This is a somewhat internal detail, but it explains
some events with `idx` equal to `0` and `input` equal to an empty string.)

```JavaScript
  for (let i = 0; i < settings.input_filepaths.length; ++i) {
    let [inputs, error] = readInputFile(settings.input_filepaths[i])
    if (error) {
      emitWarning("cannot read input file")
      finish(error)
      return
    }
    settings.inputs = settings.inputs.concat(inputs)
  }
  if (settings.inputs.length <= 0) {
    if (task.needs_input) {
      emitWarning(settings, "no input provided")
      finish(Error("value_error"))
      return
    }
    settings.inputs.push("") // empty input for input-less tests
  }
  if (settings.options.randomize_input) {
    shuffle(settings.input)
  }
```

Then, Measurement Kit iterates over all the input and runs the function
implementing the specified task on each input.

```JavaScript
  let begin = timeNow()
  for (let i = 0; i < settings.inputs; ++i) {
    let currentTime = timeNow()
    if (settings.options.max_runtime >= 0 &&
        currentTime - begin > settings.options.max_runtime) {
      emitWarning("exceeded maximum runtime")
      break
    }
    emitEvent("status.measurement_start", {
      idx: i,
      input: settings.inputs[i]
    })
    let measurement = Measurement()
    measurement.annotations = settings.annotations
    measurement.annotations.engine_name = "libmeasurement_kit"
    measurement.annotations.engine_version = mkVersion()
    measurement.annotations.engine_version_full = mkVersionFull()
    measurement.annotations.platform = platformName()
    measurement.annotations.probe_network_name = settings.options.save_real_probe_asn
                                                  ? probe_network_name : "Unknown"
    measurement.id = UUID4()
    measurement.input = settings.inputs[i]
    measurement.input_hashes = []
    measurement.measurement_start_time = currentTime
    measurement.options = []
    measurement.probe_asn = settings.options.save_real_probe_asn ? probe_asn : "AS0"
    measurement.probe_cc = settings.options.save_real_probe_cc ? probe_cc : "ZZ"
    measurement.probe_ip = settings.options.save_real_probe_ip
                              ? probe_ip : "127.0.0.1"
    measurement.report_id = report_id
    measurement.sotfware_name = settings.options.software_name
    measurement.sotfware_version = settings.options.software_version
    measurement.test_helpers = test_helpers
    measurement.test_name = test.AsOONITestName()
    measurement.test_start_time = begin
    measurement.test_verson = test.Version()
    let [test_keys, error] = task.Run(
          settings.inputs[i], settings, test_helpers)
    measurement.test_runtime = timeNow() - currentTime
    measurement.test_keys = test_keys
    measurement.test_keys.resolver_ip = settings.options.save_resolver_ip
                                          ? resolve_ip : "127.0.0.1"
    if (error) {
      emitEvent("failure.measurement", {
        failure: error.AsString(),
        idx: i,
        input: settings.inputs[i]
      })
    }
    emitEvent("measurement", {
      json_str: measurement.asJSON(),
      idx: i,
      input: settings.inputs[i]
    })
    if (!settings.options.no_file_report) {
      let error = writeReportToFile(measurement)
      if (error) {
        emitWarning("cannot write report to file")
        finish(error)
        return
      }
    }
    if (!settings.options.no_collector) {
      let error = submitMeasurementToOONICollector(measurement)
      if (error) {
        emitEvent("failure.measurement_submission", {
          idx: i,
          input: settings.inputs[i],
          json_str: measurement.asJSON(),
          failure: error.AsString()
        })
      } else {
        emitEvent("status.measurement_submission", {
          idx: i,
          input: settings.inputs[i],
        })
      }
    }
    emitEvent("status.measurement_done", {
      idx: i
    })
  }
```

Finally, Measurement Kit ends the test by closing the local results file
and the remote report managed by the OONI collector.

```JavaScript
  emitProgress(0.9, "ending the test")

  if (!settings.options.no_file_report) {
    error = closeFileReport()
    if (error) {
      emitWarning("cannot close file report")
      finish(error)
      return
    }
  }
  if (!settings.options.no_collector) {
    let error = closeRemoteReport()
    if (error) {
      emitEvent("failure.report_close", {
        failure: error.AsString()
      })
      emitWarning("cannot close remote report with OONI collector")
    } else {
      emitEvent("status.report_close", {
        report_id: report_id
      })
    }
  }

  finish()
}
```
